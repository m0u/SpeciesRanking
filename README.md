# Species-Importance Ranking Similarity Using Hybrid ML Models Across Ecosystems
Validation-based framework for ecologists â€” not a new keystone prediction tool.

This repository contains ecosystem datasets and scripts used to evaluate how closely machine-learning derived species-importance rankings reproduce expert Ecopath keystone rankings.  
The objective is not to output new influential species, but to demonstrate whether this approach is reliable enough for ecological use when expert classifications are missing or incomplete.

<p align="center">
  <img src="assets/overview.png" width="650">
</p>

---

##  Project Summary

Expert keystone ranking traditionally requires full Ecopath parameterization, long-term diet surveys, and ecological knowledge.  
Our approach tests whether ML-based ranking similarity can match expert ordering using minimal data â€” biomass + diet + network metrics.

If similarity remains consistently high across ecosystems:

âœ” ecologists may safely adopt the method in future  
âœ” full EwE reconstruction is not always necessary  
âœ” model can be extended later for keystone prediction

This study is *validation-focused* â€” similarity is the output.

---

##  Ecosystem Sets Included

### ðŸ”¹ Training Ecosystems (used for learning)
| ID | Ecosystem | Region |
|---|---|---|
| **E1** | Lake Kinneret | Israel |
| **E2** | Alto Golfo | Northern Gulf of California |
| **E3** | Celtic Sea | Northeast Atlantic |

### ðŸ”¹ Test Ecosystems (used only for similarity validation)
| ID | Ecosystem | Region |
|---|---|---|
| **E4** | *Reserved for additional test set* | â€” |
| **E5** | Cap de Creus MPA | Western Mediterranean |
| **E6** | Bay of Biscay Shelf System | Northeast Atlantic |

These are used to calculate how close ML ranking orders are to expert ground truth.

---

##  Ranking Similarity Workflow

Biomass + Diet + Network â†’ Feature Engineering â†’ { RF | LP | GraphSAGE } â†’ Ensemble â†’ Ranking Similarity


### Evaluation Metrics
- **Top-K Overlap** â€” shared top species count
- **Jaccard Index** â€” Top-K intersection vs union
- **Spearman Ï** â€” rank-ordering monotonic similarity
- **Extinction-AUC** â€” system robustness under node removal

Output = **Similarity percentage**, not a new species list.

---

##  Repository Structure

data/
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ E1_Kinneret.csv
â”‚   â”œâ”€â”€ E2_AltoGolfo.csv
â”‚   â””â”€â”€ E3_CelticSea.csv
â”œâ”€â”€ testing/
â”‚   â”œâ”€â”€ E4.csv
â”‚   â”œâ”€â”€ E5_CapDeCreus.csv
â”‚   â””â”€â”€ E6_BayOfBiscay.csv

code/
â”œâ”€â”€ features_engineering.ipynb
â”œâ”€â”€ train_RF_LP_GraphSAGE.py
â”œâ”€â”€ ensemble_ranking_similarity.py
â”œâ”€â”€ extinction_curve_analysis.py
â””â”€â”€ utils/
    â”œâ”€â”€ metrics.py
    â”œâ”€â”€ loaders.py
    â”œâ”€â”€ graph_ops.py
    â”œâ”€â”€ shap_importance.py
    â””â”€â”€ plot_utils.py

results/
â”œâ”€â”€ similarity_scores/
â”œâ”€â”€ extinction_AUC_plots/
â””â”€â”€ feature_importance/

assets/
â””â”€â”€ overview.png

README.md






##  How to Run

### 1. Install requirements
```bash
pip install -r requirements.txt
python code/train_RF_LP_GraphSAGE.py
python code/ensemble_ranking_similarity.py
python code/extinction_curve_analysis.py
/results/similarity_scores/
/results/extinction_AUC_plots/


##  Interpretation for Ecologists
| Similarity Score | Interpretation                                                              |
| ---------------- | --------------------------------------------------------------------------- |
| >0.75            | Model aligns strongly with expert keystone rankings â†’ reliable for adoption |
| 0.50â€“0.75        | Useful with partial expert oversight                                        |
| <0.50            | Ecopath re-evaluation or richer input recommended                           |

This study does not produce new keystone species.
It demonstrates method reliability, enabling future ecological adoption.

## Citation

Ghosh, M. (2025). Species Importance Ranking Similarity Using Hybrid ML.
GitHub Repository & Manuscript Release.

